{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8704LdnCDUB",
        "outputId": "d606377e-4db3-4f2c-bd46-38dd9d7e6da8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (0.0.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4) (2.5)\n",
            "Requirement already satisfied: google in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from google) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->google) (2.5)\n",
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.10/dist-packages (2.11.0)\n",
            "Requirement already satisfied: Twisted<23.8.0,>=18.9.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (22.10.0)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (41.0.5)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.2.0)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.8.1)\n",
            "Requirement already satisfied: pyOpenSSL>=21.0.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.3.0)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from scrapy) (1.6.2)\n",
            "Requirement already satisfied: service-identity>=18.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.1.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.1.2)\n",
            "Requirement already satisfied: zope.interface>=5.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (6.1)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.3.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from scrapy) (0.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from scrapy) (67.7.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from scrapy) (23.2)\n",
            "Requirement already satisfied: tldextract in /usr/local/lib/python3.10/dist-packages (from scrapy) (5.1.0)\n",
            "Requirement already satisfied: lxml>=4.4.1 in /usr/local/lib/python3.10/dist-packages (from scrapy) (4.9.3)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.10/dist-packages (from scrapy) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=36.0.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.10/dist-packages (from itemloaders>=1.0.1->scrapy) (1.0.1)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (23.1.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.5.0)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.10/dist-packages (from service-identity>=18.1.0->scrapy) (0.3.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (23.10.4)\n",
            "Requirement already satisfied: incremental>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (22.10.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.5 in /usr/local/lib/python3.10/dist-packages (from Twisted<23.8.0,>=18.9.0->scrapy) (4.5.0)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.4)\n",
            "Requirement already satisfied: requests>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (2.31.0)\n",
            "Requirement already satisfied: requests-file>=1.4 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (1.5.1)\n",
            "Requirement already satisfied: filelock>=3.0.8 in /usr/local/lib/python3.10/dist-packages (from tldextract->scrapy) (3.13.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from Automat>=0.8.0->Twisted<23.8.0,>=18.9.0->scrapy) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=36.0.0->scrapy) (2.21)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.1.0->tldextract->scrapy) (2023.7.22)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "! pip install bs4\n",
        "! pip install google\n",
        "! pip install scrapy\n",
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMJslGYmM3J8"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import os\n",
        "import itertools\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urlparse"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rndF6lJRNeA-"
      },
      "outputs": [],
      "source": [
        "# query = input()\n",
        "query = \"What recent news state about possible energy price developments over the next three months?\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.probability import FreqDist\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from dateutil.relativedelta import relativedelta\n",
        "from datetime import datetime\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "keywords_amount = 5\n",
        "current_date = datetime.now()\n",
        "\n",
        "# if f\"next {x} month\"\n",
        "\n",
        "parsed_date = current_date + relativedelta(months=2)\n",
        "formatted_date = parsed_date.strftime(\"%B %Y\")\n",
        "\n",
        "print(formatted_date)\n",
        "\n",
        "\n",
        "def remove_special_characters(queryText):\n",
        "    pattern = r'[^a-zA-Z0-9\\s]'\n",
        "    cleaned_string = re.sub(pattern,'',queryText)\n",
        "    return cleaned_string\n",
        "\n",
        "\n",
        "def prioritize_keywords(t, field_keywords):\n",
        "    keywords_with_weights = {}\n",
        "\n",
        "    for keyword in field_keywords:\n",
        "        if keyword in freq_dist:\n",
        "            keywords_with_weights[keyword] = freq_dist[keyword]*50\n",
        "\n",
        "    return keywords_with_weights\n",
        "\n",
        "\n",
        "text = remove_special_characters(query)\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "freq_dist = FreqDist(filtered_tokens)\n",
        "\n",
        "most_common = dict(freq_dist.most_common(keywords_amount))\n",
        "\n",
        "print(most_common)\n",
        "\n",
        "\n",
        "field_keywords = ['sustainable', 'stainless steel', 'recyclable', 'patent', 'energy', 'building', 'infrastructure', 'restructuring', 'measures', 'responsible', 'steel', 'efficient', 'carbon', 'clean', 'waste', 'green', 'environmental', 'innovation', 'ethical', 'materials', 'raw', 'industry', 'price', 'range', 'investment', 'money', 'inflation', 'trading', 'news']\n",
        "\n",
        "keywords_with_weights = prioritize_keywords(remove_special_characters(query), field_keywords)\n",
        "print(keywords_with_weights)\n",
        "\n",
        "combined_keywords = dict(Counter(most_common) + Counter(keywords_with_weights))\n",
        "print('ck', combined_keywords)\n",
        "\n",
        "sorted_ck = dict(Counter(dict(sorted(combined_keywords.items(), key=lambda x: x[1], reverse=True))).most_common(keywords_amount))\n",
        "print(sorted_ck)\n",
        "\n",
        "keywordList = []\n",
        "\n",
        "for keyword in sorted_ck:\n",
        "    keywordList.append(keyword)\n",
        "\n",
        "print(keywordList)"
      ],
      "metadata": {
        "id": "6s4ceuwzWt_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7y2wI2NN6xH"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from googlesearch import search\n",
        "except ImportError:\n",
        "    print(\"No module named 'google' found\")\n",
        "\n",
        "generator = search(query, tld=\"co.in\", num=6, stop=6, pause=2)\n",
        "urls = []\n",
        "\n",
        "for j in generator:\n",
        "    urls.append(j)\n",
        "    print(j)\n",
        "\n",
        "index = 0\n",
        "print(urls)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CWXmO9hjRuC8"
      },
      "outputs": [],
      "source": [
        "import scrapy\n",
        "from scrapy.spiders import CrawlSpider, Rule\n",
        "from scrapy.linkextractors import LinkExtractor\n",
        "\n",
        "website_names = [urlparse(url).netloc[4:] for url in urls]\n",
        "\n",
        "print(website_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JibgHpHSTkT4"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "class MySpider(CrawlSpider):\n",
        "    name = 'JunctionCrawling'\n",
        "    allowed_domains = website_names\n",
        "    start_urls = urls\n",
        "    allowed = keywordList\n",
        "    max_pages_per_website = 3\n",
        "    crawled_pages_per_website = {}\n",
        "    current_index = 0\n",
        "\n",
        "    rules = (\n",
        "        Rule(LinkExtractor(allow=allowed), callback='parse_item', follow=True),\n",
        "    )\n",
        "\n",
        "    def parse_item(self, response):\n",
        "        website_name = urlparse(response.url).netloc[4:]\n",
        "        print(website_name)\n",
        "\n",
        "        try:\n",
        "            _, _, files = next(os.walk(\"./\" + website_name))\n",
        "        except:\n",
        "            self.crawled_pages_per_website[website_name] = 0\n",
        "        else:\n",
        "            self.crawled_pages_per_website[website_name] = len(files)\n",
        "        print(self.crawled_pages_per_website[website_name])\n",
        "\n",
        "        if self.crawled_pages_per_website[website_name] > self.max_pages_per_website:\n",
        "            pass\n",
        "        else:\n",
        "            if all(count >= self.max_pages_per_website for count in self.crawled_pages_per_website.values()):\n",
        "                self.crawler.engine.close_spider(self, 'Reached maximum pages per website')\n",
        "\n",
        "            Path(\"./\" + website_name).mkdir(parents=True, exist_ok=True)\n",
        "            filename = \"\" + response.url.split('/')[-2] + '.html'\n",
        "            file_path = \"./\" + website_name + \"/\" + filename\n",
        "            body_content = response.xpath('//body').get()\n",
        "            with open(file_path, 'w') as f:\n",
        "                f.write(body_content)\n",
        "                if website_name not in self.crawled_pages_per_website:\n",
        "                    self.crawled_pages_per_website[website_name] = 1\n",
        "                else:\n",
        "                    self.crawled_pages_per_website[website_name] += 1\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EQI_fqkuVFsX"
      },
      "outputs": [],
      "source": [
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "process = CrawlerProcess(\n",
        "    # settings={'CLOSESPIDER_PAGECOUNT': 100,'USER_AGENT': 'my-cool-project'}\n",
        "    settings={'CLOSESPIDER_TIMEOUT': 20, 'USER_AGENT': 'SustainabLLM'}\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ZZzgKo9YFHu"
      },
      "outputs": [],
      "source": [
        "process.crawl(MySpider)\n",
        "process.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jIvt9Yps0ul"
      },
      "outputs": [],
      "source": [
        "# rm *.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf */"
      ],
      "metadata": {
        "id": "fJo85E7yGC-q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "\n",
        "# folder_names = os.listdir('./')\n",
        "best_index_scores = {}\n",
        "best_website_page = {}\n",
        "best_text = \"\"\n",
        "\n",
        "for website_name in set(website_names):\n",
        "    fileStrings = []\n",
        "    try:\n",
        "        for filename in os.listdir('./' + website_name): # iterate over all files of given website\n",
        "            f = os.path.join('./' + website_name, filename)\n",
        "            # checking if it is a file\n",
        "            if os.path.isfile(f):\n",
        "                with open(f,'r') as htmlBody:\n",
        "                    content = htmlBody.read()\n",
        "\n",
        "                soup = BeautifulSoup(content, 'html.parser')\n",
        "                tags = soup.find_all(['p', 'span'])\n",
        "\n",
        "                text = ''.join([tag.get_text() for tag in tags])\n",
        "\n",
        "                fileStrings.append(text)\n",
        "\n",
        "        vectorizer = TfidfVectorizer()\n",
        "        tfidf_matrix = vectorizer.fit_transform(fileStrings)\n",
        "\n",
        "        keyword_matrix = vectorizer.transform(keywordList)\n",
        "\n",
        "        similarity_scores = cosine_similarity(keyword_matrix, tfidf_matrix, dense_output=True)\n",
        "\n",
        "        best_match = np.mean(similarity_scores, axis=0)\n",
        "\n",
        "        best_index = best_match.argmax() # index for the best page within website\n",
        "        best_score = best_match.max()\n",
        "\n",
        "        best_website_page[website_name] = fileStrings[best_index]\n",
        "        best_text = best_text + best_website_page[website_name]\n",
        "\n",
        "        best_index_scores[website_name] = best_score\n",
        "    except:\n",
        "        print(f'{website_name} was not scraped')\n",
        "\n",
        "try:\n",
        "    # best_text = best_website_page[max(best_index_scores, key=best_index_scores.get)]\n",
        "    print(best_text)\n",
        "except:\n",
        "    best_text = \"\"\n",
        "    print('error')\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "_Zb0nNj6DhzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers\n",
        "! pip install torch"
      ],
      "metadata": {
        "id": "PF73pxZ8Rlsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"sshleifer/distilbart-cnn-6-6\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "pipe = pipeline(\"summarization\", model=model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "TBlaTt6Y_cYx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "ARTICLE_TO_SUMMARIZE = (best_text)\n",
        "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "U9YygJhWUx-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=20, max_length=20)\n",
        "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ],
      "metadata": {
        "id": "Qt6CeBcDU6TT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "API_URL = \"https://api-inference.huggingface.co/models/sshleifer/distilbart-cnn-6-6\"\n",
        "headers = {\"Authorization\": \"Bearer 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\"}\n",
        "\n",
        "def query(payload):\n",
        "\tresponse = requests.post(API_URL, headers=headers, json=payload)\n",
        "\treturn response.json()\n",
        "\n",
        "output = query({\n",
        "\t\"inputs\": best_text,\n",
        "})\n",
        "\n",
        "print('-----------------')\n",
        "print(output[0]['summary_text'])"
      ],
      "metadata": {
        "id": "P73rwV85cTdv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}